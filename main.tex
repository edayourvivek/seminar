\documentclass[14pt,a4paper,final]{extreport}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage[colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage[left=3cm,right=2cm,top=2cm,bottom=2.5cm]{geometry}
\linespread{1.2}
\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{soul}
\usepackage{ulem}
\usepackage{xcolor}
\usepackage{array, xcolor, lipsum, bibentry,fancyhdr}
\usepackage[margin=3cm]{geometry}

\usepackage{etoolbox}
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

\let\cleardoublepage\clearpage
%to remove the dots for the content page.
\makeatletter
\renewcommand{\@dotsep}{10000} 
\makeatother
\makeatletter 
%\renewcommand{\thefigure}{\@arabic\c@figure}
\makeatother




\definecolor{lightgray}{gray}{0.8}
\newcolumntype{L}{>{\raggedleft}p{0.14\textwidth}}
\newcolumntype{R}{p{0.8\textwidth}}
\newcommand\VRule{\color{lightgray}\vrule width 0.5pt}
%....................................
%....................................
%change this portion in ur tex file. copy this and paste there.
%....................................
%....................................

\titleformat{\chapter}[display]
{\normalfont\rmfamily\medium\bfseries\color{black}}
{\chaptertitlename\ \thechapter}{13pt}{\LARGE}
 
\titlespacing {\chapter}{0pc}{0pc}{.1pc}
%\titlespacing{<command>}{<left>}{<before-sep>}{<aft

\titlespacing{\section}{0pc}{.5pc}{0pc}
\titlespacing{\subsection}{0pc}{1pc}{0pc}


%........................................
%........................................
%thank u
%........................................
%........................................

\author{{{by}}\\
\textbf{VIVEK E }\\
	(Roll No:31)\\[2pt]
 }
\title{{\large\textbf{AMMINI COLLEGE OF ENGINEERING PALAKKAD}} \\
\begin{figure}[h]
	\begin{center}
\includegraphics[scale=.6]{images.jpg} \\[.1cm]
\end{center}
\end{figure}
{\large\textbf{SEVENTH SEMESTER B.TECH\\[.5 cm]}}
	{\large\textbf{SEMINAR REPORT\\[.5 cm]}}
	{\large \textbf{ON\\[.2 cm]}}
	{\large \textbf {PROGRESSIVE WEB APPS}}
		}
\date{
\textit{under the guidance of} \\[0.2cm]
	\textbf{ Mr. Sudhesh K.M} \\
	\textbf{Assistant Professor}\\
	\vspace{.1cm}
	\large\textbf{Department of Computer Science and Engineering} \\
}	
\thispagestyle{empty}

\begin{document}
\pagenumbering{gobble}
\clearpage\maketitle
\thispagestyle{empty}


\begin{center}\fontsize{17}{17} \selectfont \textbf{AMMINI COLLEGE OF ENGINEERING PALAKKAD}\end{center}
%\begin{center}\fontsize{14}{17} \selectfont \textbf{PALAKKAD}\end{center}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images.jpg}
		\vspace{.1 cm}
	\end{center}
\end{figure}
\begin{center}\fontsize{17}{17} \selectfont \textbf{\large Department of Computer Science \& Engineering\\[2cm] }\end{center}
\begin{center}
\emph{\textcolor{red}{\Large \bf CERTIFICATE}}
\end{center}
{This is to certify that the seminar report entitled {\bf{PROGRESSIVE WEB APPS}} submitted by {\bf VIVEK E}, to the {\bf Department of Computer Science and Engineering, Ammini College of Engineering, Palakkad -678613}, in partial fulfilment of the requirement for the award of B.Tech Degree in Computer Science and Engineering is a bonafide record of the work carried out by him.}
\newline
\newline
\newline
\newline
  \hspace{1cm}Mr Sudhesh K M\hspace{8.7cm}Mr Prabhu M
\newline
Co-ordinator \& Guide\hspace{6.75cm} Head Of Department
\newline
\newline
Place$:$\hspace{.5cm}Palakkad 
\newline
\renewcommand{\dateseparator}{-}
Date$:$\hspace{.5cm}\date{25-10-2019}%13-09-2010
\newline


%\thispagestyle{empty}

\newpage
\section*{\begin{center} \fontsize{14}{17} \selectfont \textbf{Acknowledgement}\end{center}}
\thispagestyle{empty}
%\vspace{1cm}

%\vspace{.4 cm }
%\begin{quote}
{                                                 
	\hspace{01.5cm}It is with great enthusiasm and learning spirit that I am bringing out this seminar report. Here I would like to mark my token of gratitude to all those who influenced me during the period of my work. I would like to express my sincere thanks to \textbf{The Management of Ammini College of Engineering, Palakkad} and \textbf{Dr.Suresh Kumar V }, The Principal Ammini College of Engineering for the facilities provided here.
\par
	\hspace{01.5cm}I express my heart-felt gratitude to Head of the Department, \textbf{Mr. Prabhu M} , Assistant Professor Department of Computer Science \& Engineering for allowing me to take up this work.
\par	
	\hspace{01.5cm}With immense pleasure I express sincere thanks to my guide and   Co-Ordinator \textbf{Mr. Sudhesh K.M} , Assistant Professor for his committed guidance, valuable suggestions and constructive criticisms. His stimulating suggestions and encouragement helped me through my seminar work. I extend my gratitude to all teachers in the Department of Computer Science and Engineering, Ammini College of Engineering, Palakkad for their support and inspiration.
\par
	\hspace{01.5cm}And above all I praise and thank the Almighty God, who showered His abundant grace on me to make this seminar a success. I also express my special thanks and gratitude to my family and all my friends for their support and encouragement.
}
%\end{quote}
%header footer....
\pagestyle{fancy}
\lfoot{{\tiny AMMINI COLLEGE OF ENGINEERING}}
\chead{}
\rfoot{\tiny{DEPT OF CSE}}
\rhead{\tiny \thepage}
\cfoot{}
\lhead{\tiny{ANIMAL COLLISION DETECTION USING CV}}
\renewcommand{\headrulewidth}{0.6pt}
\renewcommand{\footrulewidth}{0.6pt}
\newpage
\textbf{
\begin{center}
\abstractname{}
\thispagestyle{empty}
\end{center}}
The mobile apps have been reaching a huge success on the mobile market. This
opportunity attracted a lot of interested companies to have their own optimized mobile
apps for all major mobile operation systems. However, these developments are expensive
when developed natively for each mobile platform. New improvements done on the web
technologies, allowed more features and capabilities than previously was only possible on
apps that was developed natively. This started new possibilities on consolidate all
developments only on web apps, that are apps that runs on web browsers. Progressive
apps load quickly even on slow network connections, send push notifications, and have a
splash screen and an icon on the home screen. When launched from the home screen,
these apps blend into the environment; they’re top-level, full-screen, and work offline.
Progressive web apps are an interesting forward look into the future of mobile apps.

 




\tableofcontents % compile 2 times to get table of contents written
\addtocontents{toc}{\small}


\listoffigures  %compile 2 times to get list of figures written
\addtocontents{lof}{\small}


\clearpage
\pagenumbering{arabic}
\chapter{Introduction}
The mobile Web is becoming ever more capable of accessing and handling features previously only available in native and cross-platform apps. With the introduction of Progressive Web Apps (PWAs), regular Web sites can to a larger extent than before act, feel and look as any other installed app – so far particularly on an Android- based mobile device. This is enabled through a set of new concepts and requirements, advocated by Google as well-worth implementation efforts. In short, a PWA is any Web site implementing certain specific technical features. Such a Web site can, in PWA-supported browsers, be added to the home-screen of a user’s device and used offline. It looks like a regular app although being run inside a stripped-down Chrome browser, which hides all its interface artefacts.
\newline


\item 
Google states: “A Progressive Web App uses modern web capabilities to deliver an app-like user experience.” When you will get to know PWA, app development will seem not required. The mobile website itself becomes the application.So, investing in Web Technologies is an omen because it is trying to overcome the major problem possessed by mobile apps: Platform Fragmentation.
\newline
\item

The mobile web is based on web apps conforming to
standard languages like HTML5, CSS3, and JavaScript, which offer (among many) the advantage of full application portabil-
ity across platforms (e.g. Android, Apple). Even if the browser is becoming more and more a fully-fledged software platform
(e.g. the HTML5 standard provides APIs for geolocation,
accessing the camera, microphone), as of today the mobile web
struggles in providing a satisfactory experience to the user,
mainly due to the strong dependence on network conditions,
the lack of support for push notifications, and so on.

\newpage

\item In September 2015, research firm comScore released an extraordinary survey about how people actually use websites and apps. From an engagement perspective, the Web is a mile wide and an inch deep. Apps are the opposite, an inch wide and a mile deep. Apps are fast and the mobile websites are slow.
\newline
\newline
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{im1.png}
		\vspace{.1 cm}
    \caption{App and Mobile web usage}
	\end{center}
\end{figure}
\item The Web reaches wider audiences than apps. But apps dominate in time spent. So there is need of something that combines the best experiences of the web and the app. A new architecture is coming that will help to combine the best experiences of the web and the app and may finally provide the solution to building apps and websites that are fast and reliable. With Progressive Web Apps, Google has seen engagement levels of websites approach nearly that of native apps.In a nut shell, progressive Web apps start out as tabs in Chrome and become progressively more “app” like the more people use them, to the point where they can be pinned on the home screen of a phone or in the app drawer and have access to app-like properties such as notifications and offline use. Progressive Web apps are linkable with an URL, fully responsive and secure.


\begin{enumerate}
\end{enumerate}




 

\chapter{Comparative Study}
\section{Native Applications:}
 
\item
Native applications have codes that are devised specifically for a particular platform, namely Android, iOS and so on. Cross-platform codes are not possible with native apps as the codes written for one platform cannot be used in another. You may use the latest APIs, but you cannot use one platform’s code on another one. A native app would always feel right for the user because it has a mature ecosystem containing all the specific guidelines used for the OS it is developed for; ranging from swipes, app defines gestures to centrally aligned headers for iOS and left aligned headers for Android. This makes it easier for the user.
\section{Web Apps:}
\item 
Mobile web applications require web browsers to function and they are developed using HTML ,CSS and JavaScript. The program will be stored on a remote server and shows itself when the user asks for it. It is not necessary for web apps to have native codes and they can function on any operating system.
\section{Hybrid Apps:}
\item When smartphones were first released into the market, the war between native and web applications took shape and form, but along with the spoils of the war, another category was created – hybrid apps. If you want to get an app out the door as soon as possible and save time and money on developing the app, then you need hybrid apps. Hybrid app development is cross-platform app development and only one source code is used; this would be upgraded and updated to suit the purpose. Thus it combines the benefits of both native apps and web apps.
\section{Comparative factors:}
\subsection{Code Portability:}
\item
You can not port native apps from one platform to another. With web apps, you can have a single code base for any major mobile platform. This is not 100 percent portable and sometimes developers are faced with portability issues. For hybrid apps, you can reuse many of the apps from one platform to another.
\subsection{Local Storage, Offline Capability:}
Offline apps would function even when your user is not online. There is no need for the internet connection to beconstant. Local storage that retains web app data is possible with web apps, and thus it would be ideal to use web apps if you are looking for offline storage of 5MB at a time. With hybrid apps, users cannot enjoy the offline mode as much as they would want. With native apps, it would be possible for users to enjoy the capabilities of offline capability.
\subsection{Monetization:}
Every app developer seeks to come up with a ground- breaking concept when they release their app to the app store. They all expect it to bring in phenomenal success in terms of money. The possibility of monetization with native and hybrid apps would be much higher compared to web apps. The downside for native and hybrid apps is that the app store takes a percentage of all sales that you make. Whereas, for web apps, there are no commissions.
\subsection{Cost:}
Native apps often cost more to develop because they ask for specific language and tooling ecosystems, apart from the customization of code. Cost is often dependant on a number of factors and sometimes even the skill of a developer which could cost more. Web app’s functionality is based on JavaScript, CSS and HTML5. Hybrid apps are least expensive of all three.
\subsection{Time to Market Apps:}
App store is quite strict about the apps sold in their store and things get stricter for in app purchases. You have to submit an application and sometimes, wait for months to get their approval. The time to market for web apps is much smoother and simpler, whereas for the other two, you will have to wait.
\subsection{User Experience and Interaction:}
Native app provides much better accessibility features when in a native UI. You have absolute control here. Unfortunately, web apps are hindered by the capability of a web browser. For native apps, you can actually accelerate the UI performance when you enhance the capabilities of the device hardware.
\subsection{Internationalization & Localization:}
Every app development company dreams of surpassing geographical boundaries and creating software that people from anywhere can access. Internationalization & Localization for hybrid, native and web apps is excellent because the software can be designed in such a way that they can be adapted to any language without making engineering changes. These softwares can be localized by making it applicable for a particular region or language.

\chapter{Progressive web apps}


\item \textbfA Progressive web app combines the best experiences of the web and an app. They don't require any installation. The word 'progressive' comes from the relationship that the user builds with the app over time. The app loads quickly, even when the user is on bad networks. It can send relevant push notifications to the user and has an icon on the home screen and loads as top-level, full screen experience.

\item A progressive web app can be defined as :

\item\textbf{Reliable -} Load instantly and never show the downasaur, even in uncertain network conditions.

\item \textbf{Fast -} Respond quickly to user interactions with silky smooth animations and no janky scrolling.
\item \textbf{Engaging -} Feel like a natural app on the device, with an immersive user experience.
\chapter{Characteristics}
\item 
There are certain characteristics that define PWAs, and that differentiate them from regular Web sites and native or cross-platform mobile apps. The main differentiator between a regular Web site and a PWA is the added functionality and User Experience (UX) the latter provides. Where a regular Web site requires the user to open a browser, type in a URL and wait for all content to be downloaded on every visit, effectively preventing an offline experience, a PWA only requires these steps for the first visit. After a home screen installation, all necessary static files, including HTML, CSS, JavaScript, images and fonts for the Web site, are now stored on the user’s phone, ready to be used offline. All dynamic data can be cached for offline (or low-connectivity) use, and re-fetched when needed, e.g. when new data is available and the phone is on a decent network connection.
Where a regular Web site would be wrapped in a browser (e.g. Chrome Android) with visible browser artefacts (such as address bar and menus), a PWA will similarly run in a browser instance, but without those artefacts [58]. Thus, a PWA will look similar to a regular app. If a PWA is styled correctly, following the design guidelines of each mobile platform, telling apart a regular native or cross-platform app and a PWA from the appearance would be challenging.


\chapter{Core building blocks of Progressive web app}
\section{Application shell}
\itemAn Application shell (or app shell) architecture is one way to build a Progressive Web App that reliably and instantly loads on your users' screens, similar to what you see in native applications.

The app "shell" is the minimal HTML, CSS and JavaScript required to power the user interface and when cached offline can ensure instant, reliably good performance to users on repeat visits. This means the application shell is not loaded from the network every time the user visits. Only the necessary content is needed from the network.

For single-page applications with JavaScript-heavy architectures, an application shell is a go-to approach. This approach relies on aggressively caching the shell (using a service worker) to get the application running. Next, the dynamic content loads for each page using JavaScript. An app shell is useful for getting some initial HTML to the screen fast without a network.




\chapter{Brief Overview and Advantages of HOG and Cascade Classifier}
\item A histogram of oriented gradients (HOG) is used in computer vision applications for detecting objects in a video or image, which by definition is a feature descriptor.
\item First the input image is given to color normalization block. Color normalization is used for object recognition on color images when it is important to remove all intensity values from the picture while preserving color values. After color normalization, the second step of calculation is the computation of the gradient values. The most common method is to apply the 1D centered point discrete derivative mask in both the horizontal and vertical directions. Specifically, this method requires filtering the grey scale image with the following filter kernels:

\begin{equation*} D_{X}=[-1\quad 0\quad 1]\quad \text {and}\quad D_{Y}=\left [{\begin{array}{c} 1\\ 0\\ -1 \end{array}}\right ] \end{equation*}

\item So, given an image I, we obtain the x and y derivatives using a convolution operation: 
\newline
\text{I}_{\mathrm {X}} = \text {I} ^\ast \text {D}_{\mathrm {X}}
\newline
\text{I}_{\mathrm {Y}} = \text {I} ^\ast \text {D}_{\mathrm {Y}}

\item The magnitude of the gradient is given by
\textbf{$\ |G|$= $\sqrt{I^{2}_{X}+I^{2}_{Y}}$},
\newline and orientation of the gradient is given by \textbf{$\theta$=arctan (IY/IX).}
\item The next step of calculation involves creating the cell histograms. Each pixel within the cell casts a weighted vote for an orientation-based histogram channel based on the values found in the gradient computation. 
\newpage The cells themselves are rectangular, and the histogram channels are evenly spread over 0 to 180 degrees or 0 to 360 degrees, depending on whether the gradient is “unsigned” or “signed”. As for the vote weight, pixel contribution can be the gradient magnitude itself, or the square root or square of the gradient magnitude.

\item To account for changes in illumination and contrast, the gradient strengths must be locally normalized, which requires grouping the cells together into larger, spatially-connected blocks which are the next step. The HOG descriptor is then the vector of the components of the normalized cell histograms from all of the block regions. These blocks typically overlap, meaning that each cell contributes more than once to the final descriptor.

\item Two main block geometries exist: rectangular R-HOG blocks and circular C-HOG blocks. R-HOG blocks are square grids, represented by three parameters: the number of cells per block, the number of pixels per cell, and the number of channels per cell histogram. There are different methods for block normalization. Let v be the non-normalized vector containing all histograms in a given block, \[||vk||\] be its k-norm for k = 1, 2 and e be some small constant (whose value will not influence the results). Then the normalization factor can be one of the following:

\begin{align*} \text {L2-norm:}~f=&\frac {v}{\sqrt {\Vert v\Vert ^{2}_{2}+e^{2}}} \\ \text {L1-norm:}~f=&\frac {v}{\Vert v\Vert _{1}+e} \\ \text {L1-sqrt:}~f=&\sqrt {\frac {v}{\Vert v\Vert _{1}+e}} \end{align*}

Finally, the image goes to cascade classifier for classification of the object. HOG descriptor is mainly suitable for animal detection in video or images due to some key advantages compared to other descriptors. First, it operates on local cells, so it is invariant to geometric and photometric transformations. Secondly, coarse (spatial) sampling, fine orientation sampling, and strong local photometric normalization allow different body movement of animals to be overlooked if they maintain a roughly upright position.

Cascading is a concatenation of various classifiers (group based learning). The technique involves taking all the data collected from the output of the first classifier as a supplementary data for the next classifier in the group [33]. The key advantages of boosted cascade classifiers over monolithic classifiers are that it is a fast learner and requires low computation time. Cascading also eliminates candidates (false positives) early on, so later stages don’t bother about them.

\item Each filter rejects non-object windows and let object windows pass to the next layer of the cascade. A window is considered as an object if and only of all layers of the cascade classifies it as object [33]. The filter i of the cascade is designed to

\item 1) Reject the possibly large number of non-object windows.

\item 2)To allow possible large number of object windows for quick evaluation.
\newline
\newline

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/6.png}
		\vspace{.1 cm}
		\caption[Hog Algorithm]{Hog Algorithm}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/7.jpeg}
		\vspace{.1 cm}
		\caption [Block Normalisation scheme of HOG]{Block Normalisation scheme of HOG}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.8]{images/8.png}
		\vspace{.5cm}
		\caption [Block Normalisation scheme of HOG]{Block Normalisation scheme of HOG}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/9.jpeg}
		\caption [Architecture of animal detection and collision avoidance]{Architecture of animal detection and collision avoidance}
	\end{center}
\end{figure}


\chapter{Research Methodology}
\item The video is taken from a forward-facing optical sensor (camera) in which a moving animal is present apart from other stationary and non-stationary objects. This video is stored in the computer and converted into different frames. Then we are doing pre-processing steps to enhance the image. For feature extraction and learning of the system, they are using a combination of HOG and boosted cascade classifier for animal detection. All the image processing techniques are implemented in OpenCV software. Once the animal gets detected in the video, the next step is to find the distance of the animal from the testing vehicle and then alert the driver so that he can apply the brakes or perform any other necessary action which is displayed on command prompt as a message. Depending on the distance of the animal from the camera mounted vehicle, three kinds of messages (indication) are given to the driver i.e. animal very near, if animal is very near to the vehicle, animal little far, if the animal is little far from the vehicle and very far, if the animal is very far and at a safe distance from the vehicle.

\newline
\newline
\chapter{Procedure for Training and Testing}
\item India has more than 20 varieties of cow found in different states of India such as Gir, Sahiwal, Red Sindhi, Sahiwal, Kankrej, Dandi, and others. We have collected and added all the varieties of a cow in the database for training the system. Following is the proposed procedure for training and testing of the data for animal detection:

\item 1) Collect all positive and negative images in the data folder.

\item 2) Generate Annotation

\item 3) Create sample i.e. generate .vec file

\item 4) Train data and generating XML file. Table 1 shows the parameters used set during training of the system

\item 5) Testing

\item The average time it took to generate a cascade on Intel(R) Core(TM) i5-2430M CPU 2.40GHz, 4GB RAM was almost 14 hours.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/10.jpeg}
		\caption [Positive Samples]{Positive Samples}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/11.jpeg}
		\caption [Negative Samples]{Negative Samples}
	\end{center}
\end{figure}


\chapter{Distance Calculation of the Detected Animal}
\item The video is taken and converted into frames (image of size 640x480 ). Following is the procedure for calculating the distance of the detected animal from the camera-mounted vehicle:

\item Image resolution is 640x480

X range is 0 to 640

Y range is 0 to 480

Let the right bottom coordinate of the detected cow be (x, y). Then the distance of cow from the lower edge (car/camera) is 480 – y.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.2]{images/13.png}.
		\caption [Distance Calculation]{Distance Calculation}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.5]{images/14.jpeg}
		\caption [Same object kept at different positions(depth) from camera center]{Same object kept at different positions(depth) from camera center}
		\vspace{.5cm}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.5]{images/12.jpg}
		\caption [Positive Samples]{Positive Samples}
	\end{center}
\end{figure}

\chapter{Conversion From Pixels to Meters}
\item There is some relationship between the depth of the object in pixel and depth in real world units (meters) from the camera mounted vehicle once the object (animal) gets detected in the frame. As the depth of the object in meters from the camera mounted vehicle increases (size of the object decreases), the depth in pixels also increases and vice versa. This hinted to find a relationship between the depth of the object in pixels and meters. Once the camera position in the car and height of the camera from the ground was fixed (camera calibration done), they took different images of the same object kept at various depths from the camera centre. The depth of the object from the camera centre in meters was known to us.
\item It can be noted the corresponding depth of the object in pixels. Table shown represents the relation between pixels and meters. Graph of depth in meters versus depth in pixels was plotted in Excel  and the best fitting second order polynomial equation is
\newline\item \textbf \{y=0.0323x2+22.208x+1.3132(1)\}
\newline \item where y is the depth in pixels and x is depth in meters.
\newline
\newline

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.4]{images/15.png}
		\vspace{.1 cm}
		\caption[Relationship between pixels and meters]{Relationship between pixels and meters}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.1]{images/16.png}
		\vspace{.1 cm}
		\caption[Graph of depth(meters) versus depth(pixels)]{Graph of depth(meters) versus depth(pixels)}
	\end{center}
\end{figure}


\chapter{Testing of Actual Distance Versus Calculated Distance}
\item They took two images of a cow in which we knew the depth of the cow in meters from the camera-mounted vehicle(Fig 10.3). Then they calculated the depth using the technique as mentioned earlier. Table shows the results of actual depth and calculated depth. The error is very less (less than 2 percent).

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.2]{images/17.jpeg}
		\vspace{.1 cm}
		\caption[Testing images(depth in meters already known)]{Testing images(depth in meters already known)}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.5]{images/18.jpg}
		\vspace{.1 cm}
		\caption[Actual depth versus Calculated depth]{Actual depth versus Calculated depth}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.2]{images/19.jpeg}
		\vspace{.1 cm}
		\caption[Camera mounted vehicle]{Camera mounted vehicle}
	\end{center}
\end{figure}

\chapter{Experiments and Result Analysis}
\item Using HOG descriptors which are feature descriptors that are used in computer vision and image processing for the purpose of object detection. For object classification,they are using boosted cascade classifiers. A good source for the animal images is the KTH dataset and NEC dataset that included pictures of cows and other animals. Some more animal images have been clicked (during different weather conditions i.e. morning, afternoon and evening) for creating a robust database of almost 2200 images consisting of positive images in which the target animal is present and negative images in which there is no target animal for feature extraction and for training the classifier. After the classifier is trained and the detection system is built, they tested the same on various videos.

Videos have been taken using a camera having a frame rate of 30fps mounted on the testing vehicle. Hardware used in our experiment is ASUS x53s, Intel(R) Core(TM) i5-2430M CPU 2.40GHz, 4GB RAM. Software used is MicrosoftVisual Studio 10 Professional, OpenCV 2.4.3, 64 bit operating running under Windows 7.

Parameters which are necessary for checking the performance of the test/classifier are Sensitivity (True Positive Rate), Specificity (True Negative Rate) and Accuracy which are given as:
\newline \textbf {\[Sensitivity=TP/(TP + FN)\] } 
\textbf{\[Specificity=TN/(TN + FP) \]}
\textbf{\[Accuracy=(TN + TP)/(TN + TP + FN + FP)\] }
 \item Here in above equations, TN stands for true negative; TP stands for true positive; FN stands for false negative, and FP stands for false positive. True positive (TP) and true negative (TN) are the most relevant and correct parameters of classification. False Positive indicates that the animal is detected in the frame (video) even though the animal is absent in that particular frame at that given location. False Negative (FN) indicates that there is no animal present in the frame (video) even though the animal is present in that particular frame.

In the implemented animal detection system, there were 640 frames in which 105 frames are showing animal detected i.e. rectangular box even though there is no animal present in those frame at those places. So, false positive in this case turns out to be 105 and true negative turns out to be 535. Similarly out of 640 frames, 125 frames are showing no animal detected i.e. no rectangular box even though animals are present in that frame. So false negative turns out to be 125 and true positive turns out to be 515. Substituting the above parameter values in equation (2), (3) and (4), they got sensitivity close to 80.4\%, specificity close to 83.5\% and accuracy of the classifier close to 82.5\%.

Figure 11.3 shows the on-board camera with the processing and display system inside the car on the dashboard side. They performed extensive experiments and spent so many hours testing the system in different weather conditions on the road. Figure 12.1 shows the true positive scenario wherein in the video, animal (cow) is present and our proposed system correctly detects it and gives an indication (box). Similarly, Figure 12.2  shows a false positive case wherein animal (cow) is detected in the video by the system even though it is absent in that particular frame at that given location. Figure 12.3 shows a false negative case wherein though the animal (cow) is present in the video; Figure 12.4  shows animal detected in the morning condition with the experimental camera mounted vehicle stationary i.e. at 0 kmph speed. Figure 12.5 shows animal detected in the evening state at a distance of 11 meters from the camera mounted testing vehicle with the vehicle moving at a speed of 60 kmph. Figure 12.6 shows multiple animals detected in one of the testing videos at a distance of 17 meters from the camera mounted vehicle. Training and testing on large data sets will improve the detection rate and accuracy of the classifier.

The average processing (computation) time with the proposed image processing method is 100ms (10 frames per second) which can be still be shortened using Nvidia’s CUDA processor. According to an article, the term response time or brain reaction time of the drivers in traffic engineering literature is composed of mental processing time, movement time and mechanical response time. As per the “two-second rule” which is usually a rule of thumb suggests that a driver should ideally stay at least two seconds behind any object that is in front of the driver’s vehicle. The two-second rule is useful as it can be applied to any speed and provides a simple and common-sense way of improving road safety. So if we go with “two-second rule”, clearly from Table (speed-distance relation as well as actual time (onboard) available for the driver to responds), it indicates that when the speed of the vehicle is between 30 to 35 kmph, the driver gets some time to apply brakes and can avoid a collision. Anything above this speed, though the alert signal is available the driver won’t be able to avoid a collision.
\newline
\newline

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/20.jpeg}
		\vspace{.1 cm}
		\caption[True positive case]{True positive case}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/21.jpeg}
		\vspace{.1 cm}
		\caption[False positive case]{False positive case}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/22.jpeg}
		\vspace{.1 cm}
		\caption[False negative case]{False negative case}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/23.jpeg}
		\vspace{.1 cm}
		\caption[Animal detection at 0Kmph speed(morning condition)]{Animal detection at 0Kmph speed(morning condition)}
	\end{center}
\end{figure}
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/25.jpeg}
		\vspace{.1 cm}
		\caption[Animal detected at distance of 11kmph in the evening with speed of 60Kmph]{Animal detected at distance of 11kmph in the evening with speed of 60Kmph}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.7]{images/26.jpeg}
		\vspace{.1 cm}
		\caption[Multiple animals detected in one of the testing video]{Multiple animals detected in one of the testing video}
	\end{center}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=.4]{images/30.jpg}
		\vspace{.1 cm}
		\caption[Speed-distance relation]{Speed-distance relation}
	\end{center}
\end{figure}

\chapter{Achievements with Respect to Objectives}
\item 1) Algorithm developed is working properly and able to detect an animal in different conditions on roads and highways.

\item 2) Estimation of animal distance from the testing vehicle is done. Maximum detecting distance of the animal from the camera mounted vehicle was found to be 20 meters.

\item 3) Speed analysis (different speeds like 20, 30, 35, 40, 50, 60 kmph) is implemented and tested.

\item 4) Alert signal to the driver is available.

\chapter{Limitations and Future Scope}
\item Though the proposed system can detect the animals (cow) on roads and highways as well as gives alert to the driver, it has some limitations too. The proposed system can detect animal up to a distance of 20 meters only when a vehicle is stationary. The system can prevent collision of the vehicle with the animal when driving at a speed in between 30 to 35 kmph. Beyond this speed, though animal gets detected time is not sufficient to prevent animal-vehicle collision.

\item Some means or method of increasing the detecting distance of the animal from the camera mounted vehicle needs to be done so that driver gets sufficient time for applying brakes or take any other action for preventing the collision which may be solved using high-end resolution cameras or radar. No effort has been made to detect animals during the night, which is expected to be done in the future scope of study and research.

\chapter{Conclusion}
An efficient automatic animal detection and warning system can help drivers in reducing the number of collisions occurring between the animal and the vehicle on roads and highways. In this paper, they discussed the necessity of automatic animal detection system and the algorithm for animal detection based on HOG and cascade classifier. The algorithm can detect an animal in different conditions on highways. The proposed system achieves an accuracy of almost 82.5\% regarding animal (cow) detection. Estimation of approximate animal distance from the testing vehicle is also done. Though the proposed work has been focused on automatic animal detection in context to Indian highways, it will work in other countries also. The proposed method can easily be extended for detection of other animals too after proper training and testing. The proposed system can be used with other available, efficient pedestrian and vehicle detection systems and can be offered as a complete solution (package) for preventing collisions and loss of human life on highways..

\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\begin{thebibliography}{9}
\bibitem{item1}
{\fontsize{13pt}{8.4pt}\selectfont \textit{NHTSA\ 2020  Report},\ \ accessed\ \ on\ \ Sep.\  8,  2015.  [Online].  Available: \href{http://www.nhtsa.gov/nhtsa/whatis/planning/2020Report/}{http://www.nhtsa.gov/nhtsa/whatis/planning/2020Report/} 2020report.html\par}\par

\bibitem{item2}
{\fontsize{13pt}{8.4pt}\selectfont \textit{Global Status Report on Road Safety 2013. Executive Summary}, World Health Org., Geneva, Switzerland, Oct. 2013.\par}\par

\bibitem{item3}
{\fontsize{13pt}{8.4pt}\selectfont C. J. L. Murray and A. D. Lopez, ‘‘Alternative projections of mortality and disability by cause 1990–2020: Global burden of disease study,’’ \textit{Lancet}, vol. 349, pp. 1498–1504, May 1997.\par}\par

\bibitem{item4}
{\fontsize{13pt}{8.4pt}\selectfont \textit{Ministry of Home Affairs. Accidental Deaths $\&$  Suicides in India 2012}, Nat. Crime Records Bureau, New Delhi, India, Jun. 2013.\par}\par

\bibitem{item5}
{\fontsize{13pt}{8.4pt}\selectfont National\ Highways\ Authority of  India.  \textit{Information\ \ About  Indian\  Road Network}, accessed on Jun. 3, 2010. [Online]. Available: \href{http://www.nhai.org/roadnetwork.htm}{http://www.nhai.org/roadnetwork.htm}\par}\par

\bibitem{item6}
{\fontsize{13pt}{8.4pt}\selectfont J. Padmanaban, R. Rajaraman, G. Stadter, S. Narayan, and B. Ramesh, ‘‘Analysis of in-depth crash data on indian national highways and impact of road design on crashes and injury severity,’’ in \textit{Proc. 4th Int. Conf. ESAR ‘Expert Symp. Accident Res.’},\  Hanover,\ \ Germany,\  Sep.  2010, pp. 170–183.\par}\par

\bibitem{item7}
{\fontsize{13pt}{8.4pt}\selectfont \textit{Ministry of Home Affairs. Accidental Deaths $\&$  Suicides in India 2006}, Nat. Crime Records Bureau, New Delhi, India, Jun. 2007.\par}\par

\bibitem{item8}
{\fontsize{13pt}{8.4pt}\selectfont \textit{Accident Research Study on the Ahmedabad–Gandhinagar Highway for the Duration February 2014 to January 2015}, JP Res. India Pvt Ltd., Mar. 2015.\par}\par

\bibitem{item9}
{\fontsize{13pt}{8.4pt}\selectfont S. Sharma and D. J. Shah, ‘‘A brief overview on different animal detec- tion method,’’ \textit{Signal Image Process., Int. J.}, vol. 4, no. 3, pp. 77–81, Jun. 2013.\par}\par

\bibitem{item10}
{\fontsize{13pt}{8.4pt}\selectfont M. Fabre-Thorpe, A. Delorme, C. Marlot, and S. Thorpe, ‘‘A limit to the speed of processing in ultra-rapid visual categorization of novel natural scenes,’’ \textit{J. Cognit. Neurosci.}, vol. 13, no. 2, pp. 171–180, Mar. 2001.\par}\par

\bibitem{item11}
{\fontsize{13pt}{8.4pt}\selectfont T. Burghardt and J. Calic, ‘‘Analysing animal behaviour in wildlife videos using face detection and tracking,’’ \textit{IEE Proc.-Vis., Image Signal Process.}, vol. 153, no. 8, pp. 305–312, Jun. 2006.\par}\par

\bibitem{item12}
{\fontsize{13pt}{8.4pt}\selectfont D. Walther, D. Edgington, and C. Koch, ‘‘Detection and tracking of objects in underwater video,’’ in \textit{Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)}, Washington, DC, USA, Jun./Jul. 2004, pp. 544–549.\par}\par

\bibitem{item13}
{\fontsize{13pt}{8.4pt}\selectfont J. C. Nascimento and J. S. Marques, ‘‘Performance evaluation of object detection algorithms for video surveillance,’’ \textit{IEEE Trans. Multimedia}, vol. 8, no. 4, pp. 761–774, Aug. 2006.\par}\par

\bibitem{item14}
{\fontsize{13pt}{8.4pt}\selectfont N. Kawasaki, ‘‘Parametric study of thermal and chemical non equilibrium nozzle flow,’’ M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.\par}\par

\bibitem{item15}
{\fontsize{13pt}{8.4pt}\selectfont S. L. Hannuna, N. W. Campbell, and D. P. Gibson, ‘‘Identifying quadruped gait in wildlife video,’’ in \textit{Proc. IEEE Int. Conf. Image Pro- cess. (ICIP)}, Genoa, Italy, Sep. 2005, pp. 713–716.\par}\par

\bibitem{item16}
{\fontsize{13pt}{8.4pt}\selectfont D. P. Gibson, N. W. Campbell, and B. T. Thomas, ‘‘Quadruped gait analysis using sparse motion information,’’ in \textit{Proc. Int. Conf. Image Process. (ICIP)}, New York, NY, USA, Sep. 2003, pp. 333–336.\par}\par

\bibitem{item17}
{\fontsize{13pt}{8.4pt}\selectfont H. Ragheb, S. Velastin, P. Remagnino, and T. Ellis, ‘‘Human action recognition using robust power spectrum features,’’ in \textit{Proc. 15th Int. Conf.\ Image Process.  (ICIPO)},\ \ San\ \ Diego,\ \ CA,  USA,  Oct.  2008,\  pp. 753–756.\par}\par

\bibitem{item18}
{\fontsize{13pt}{8.4pt}\selectfont F. A. Wichmann, J. Drewes, P. Rosas, and K. R. Gegenfurtner, ‘‘Animal detection in natural scenes: Critical features revisited,’’ \textit{J. Vis.}, vol. 10, no. 6, pp. 1–27, 2010.\par}\par

\bibitem{item19}
{\fontsize{13pt}{8.4pt}\selectfont P. Viola and M. J. Jones, ‘‘Robust real-time face detection,’’ \textit{Int. J. Comput. Vis.}, vol. 57, no. 2, pp. 137–154, 2004.\par}\par

\bibitem{item20}
{\fontsize{13pt}{8.4pt}\selectfont D. Ramanan, D. A. Forsyth, and K. Barnard, ‘‘Building models of animals from video,’’\  \textit{IEEE Trans. Pattern Anal. Mach. Intell.}, vol. 28, no. 8,\ \  pp. 1319–1334, Aug. 2006.\par}\par

\bibitem{item21}
{\fontsize{13pt}{8.4pt}\selectfont M. S. Zahrani, K. Ragab, and A. U. Haque, ‘‘Design of GPS-based system to avoid camel-vehicle collisions: A review,’’ \textit{Asian J. Appl. Sci.}, vol. 4, no. 4, pp. 362–377, 2011.\par}\par

\bibitem{item22}
{\fontsize{13pt}{8.4pt}\selectfont S. Shaikh, M. Jadhav, N. Nehe, and U. Verma, ‘‘Automatic animal detection and warning system,’’ \textit{Int. J. Adv. Found. Res. Comput.}, vol. 2, pp. 405–410, Jan. 2015.\par}\par

\bibitem{item23}
{\fontsize{13pt}{8.4pt}\selectfont V. Mitra, C.-J. Wang, and G. Edwards, ‘‘Neural network for LIDAR detection of fish,’’\  in \textit{Proc. Neural Netw.\  Int. Joint Conf.}, Jul. 2003,\ \ \  pp. 1001–1006.\par}\par

\bibitem{item24}
{\fontsize{13pt}{8.4pt}\selectfont S. Sharma and D. Shah, ‘‘Real-Time automatic obstacle detection and alert system for driver assistance on Indian roads,’’ \textit{Indonesian J. Elect. Eng. Comput. Sci.}, vol. 1, no. 3, pp. 635–646, Mar. 2016.\par}\par

\bibitem{item25}
{\fontsize{13pt}{8.4pt}\selectfont M. Zeppelzauer, ‘‘Automated detection of elephants in wildlife video,’’ \textit{EURASIP\ \ J.\  Image  Video\ \  Process.}, vol.\ \ 46,\ \ p.\  46,  Aug.  2013,\ \  doi: 10.1186/1687-5281-2013-46.\par}\par
\end{thebibliography}


\end{document}
